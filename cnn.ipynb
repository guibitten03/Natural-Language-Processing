{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks for Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m'\u001b[39m\u001b[39mbasic_english\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Construindo o vocabulário a partir dos dados de treinamento\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m vocab \u001b[39m=\u001b[39m build_vocab_from_iterator(\u001b[39mmap\u001b[39;49m(tokenizer, train_iter), specials\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39m<unk>\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      9\u001b[0m vocab\u001b[39m.\u001b[39mset_default_index(vocab[\u001b[39m\"\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[39m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m     99\u001b[0m     counter\u001b[39m.\u001b[39mupdate(tokens)\n\u001b[1;32m    101\u001b[0m specials \u001b[39m=\u001b[39m specials \u001b[39mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/torchtext/data/utils.py:46\u001b[0m, in \u001b[0;36m_basic_english_normalize\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_basic_english_normalize\u001b[39m(line):\n\u001b[1;32m     25\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    Basic normalization for a line of text.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Normalization includes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    Returns a list of tokens after splitting on whitespace.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     47\u001b[0m     \u001b[39mfor\u001b[39;00m pattern_re, replaced_str \u001b[39min\u001b[39;00m _patterns_dict:\n\u001b[1;32m     48\u001b[0m         line \u001b[39m=\u001b[39m pattern_re\u001b[39m.\u001b[39msub(replaced_str, line)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Baixando e carregando o dataset AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# Criando um tokenizador\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Construindo o vocabulário a partir dos dados de treinamento\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo da CNN\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, kernel_sizes, hidden_dim, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in kernel_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # .unsqueeze(1) serve para criar uma dimensão a mais na posição passada por parametros.\n",
    "        # É principalmente usado para concatenar tensores que representam palavras.\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_output = F.relu(conv(embedded))\n",
    "            pool_output = F.max_pool2d(conv_output, (conv_output.shape[2], 1))\n",
    "            conv_outputs.append(pool_output.squeeze(3))\n",
    "        cat_outputs = torch.cat(conv_outputs, 1)\n",
    "        fc_output = self.fc(cat_outputs)\n",
    "        fc_output = F.relu(fc_output)\n",
    "        out = self.out(fc_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo hiperparâmetros do modelo\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 32\n",
    "NUM_FILTERS = 100\n",
    "KERNEL_SIZES = [3, 4, 5]\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Instanciando o modelo\n",
    "model = TextCNN(VOCAB_SIZE, EMBED_DIM, NUM_FILTERS, KERNEL_SIZES, HIDDEN_DIM, NUM_CLASSES)\n",
    "\n",
    "# Definindo a função de perda e o otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch.text, batch.label\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = accuracy(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando a acurácia do modelo\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
